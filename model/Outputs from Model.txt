This is a basic file we can use to output the results of our tests as we modify the model.

Goals (for this branch at least): Get as high AUC, Precision, and Recall as possible

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(128, activation='relu')(inputs)
x = Dense(1024, activation='relu')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  89.76372838020325
Mean precision:  44.914261028170586
Mean recall:  18.77666436135769

Conclusion, attempt to focus towards raising precision and recall

<<<<<<< Updated upstream
Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(128, activation='relu')(inputs)
x = Dense(1024, activation='relu')(x)
x = Dense(2048, activation='relu')(x)
x = Dense(4096, activation='relu')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  90.53285896778107
Mean precision:  34.698971286416054
Mean recall:  21.415214091539383

Conclusion, more layers not necessarily better. Results for this calculation took much longer, did not effect AUC, reduced precision and slightly increased recall

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(128, activation='relu')(inputs)
x = Dense(4096, activation='relu')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  90.88915467262268
Mean precision:  90.68334341049194
Mean recall:  14.268363118171692

Conclusion, says high precision, but direct outputs show that the results barely include other numbers

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(64, activation='relu')(inputs)
x = Dense(512, activation='relu')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  90.68036794662476
Mean precision:  80.66275000572205
Mean recall:  13.508799970149994

Conclusion, model is still overcommitting to just saying no allergies

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(128, activation='relu')(inputs)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  77.13840126991272
Mean precision:  49.387765526771545
Mean recall:  14.162589311599731

Conclusion, at the very least, two layers are necessary to capture complexity

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(1024, activation='relu')(inputs)
x = Dense(4096, activation='relu')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  90.59560894966125
Mean precision:  96.45328044891357
Mean recall:  13.529437184333801

Conclusion, not necessarily better or worse than others at this point

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(1024, activation='sigmoid')(inputs)
x = Dense(4096, activation='sigmoid')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  91.88498258590698
Mean precision:  100.0
Mean recall:  13.413797914981842

Conclusion, sigmoid is not as effective, or maybe worse than relu

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(1024, activation='softmax')(inputs)
x = Dense(4096, activation='softmax')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  58.864569664001465
Mean precision:  100.0
Mean recall:  13.413797914981842

Conclusion, softmax is the worst activation function of those tested so far

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(1024, activation='relu')(inputs)
x = Dense(4096, activation='relu')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss=keras.losses.BinaryCrossentropy(reduction='none'), optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(), keras.metrics.Recall()])

Model outputs:

Mean area under curve:  85.9045946598053
Mean precision:  99.98990416526794
Mean recall:  13.413797914981842

Conclusion, reduction affects AUC, but impact on other evaluators is minimal

Model base code:

n_classes = 731
input_shape = (40,)
inputs = keras.Input(input_shape)
x = Dense(128, activation='relu')(inputs)
x = Dense(1024, activation='relu')(x)
x = Dense(4096, activation='relu')(x)
predictions = Dense(n_classes, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = predictions)
model.compile(loss=keras.losses.BinaryCrossentropy(reduction='none'), optimizer='sgd')

Manually set threshold to 0.4

Model outputs:

Mean area under curve:  0.34773898
Mean precision:  0.67009723
Mean recall:  0.15775342

Conclusion, threshold lowers AUC and Precision, but does not have significant enough effect on recall




=================================================================================================
Noah Stuff
=================================================================================================

    n_classes = 731
=======


=================================================
                Noah's Section: 
=================================================
cv = KFold(n_splits=10, shuffle=True, random_state=1023)

n_classes = 731
>>>>>>> Stashed changes

    # Input layer
    input_shape = (40,)
    inputs = keras.Input(input_shape)

    # Hidden layers
<<<<<<< Updated upstream
    x = Dense(128, activation='sigmoid')(inputs)
    x = Dense(512, activation = 'sigmoid')(x)
#     x = Dense(1024, activation = 'sigmoid')(x)

predictions = Dense(n_classes, activation='sigmoid')(x)
    
    model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss=keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True, alpha = 0.25, gamma = 12), optimizer=keras.optimizers.Adam(learning_rate=0.1))
    
Area under the curve:  0.3488372
Precision:  0.025698263
Recall:  0.95364505
Precision/Recall avg:  0.25201082

Baseline
__________________________________________________________________________________________

    n_classes = 731
=======
    x = Dense(128, activation='relu')(inputs)
#     x = Dense(256, activation='relu')(x)
#     x = Dense(512, activation='relu')(x)
#     x = Dense(1024, activation='relu')(x)
#     x = Dense(2048, activation='relu')(x)


    # Output layer - use multilabel classification
    predictions = Dense(n_classes, activation='sigmoid')(x)
    
    #Tie model together
    model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(),keras.metrics.Recall()])
    


Mean area under curve:  83.67926836013794
Mean precision:  47.664906084537506
Mean recall:  15.74330985546112
____________________________________________________

cv = KFold(n_splits=10, shuffle=True, random_state=1023)

n_classes = 731
>>>>>>> Stashed changes

    # Input layer
    input_shape = (40,)
    inputs = keras.Input(input_shape)

    # Hidden layers
<<<<<<< Updated upstream
    x = Dense(128, activation='sigmoid')(inputs)
    x = Dense(512, activation = 'sigmoid')(x)
#     x = Dense(1024, activation = 'sigmoid')(x)

predictions = Dense(n_classes, activation='sigmoid')(x)
    
     model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss=keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True, alpha = 0.05, gamma = 12), optimizer=keras.optimizers.Adam(learning_rate=0.1))
   
    
Area under the curve:  0.3488372
Precision:  0.048142716
Recall:  0.7883679
Precision/Recall avg:  0.24441543

Lowered alpha value, resulted in less predictions from the machine. 
__________________________________________________________________________________________

    n_classes = 731
=======
    x = Dense(128, activation='relu')(inputs)
    x = Dense(256, activation='relu')(x)
#     x = Dense(512, activation='relu')(x)
#     x = Dense(1024, activation='relu')(x)
#     x = Dense(2048, activation='relu')(x)


    # Output layer - use multilabel classification
    predictions = Dense(n_classes, activation='sigmoid')(x)
    
    #Tie model together
    model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(),keras.metrics.Recall()])
    


Mean area under curve:  84.9429315328598
Mean precision:  68.64696890115738
Mean recall:  13.59891653060913
____________________________________________________

cv = KFold(n_splits=10, shuffle=True, random_state=1023)

n_classes = 731
>>>>>>> Stashed changes

    # Input layer
    input_shape = (40,)
    inputs = keras.Input(input_shape)

    # Hidden layers
<<<<<<< Updated upstream
    x = Dense(128, activation='sigmoid')(inputs)
    x = Dense(512, activation = 'sigmoid')(x)
    x = Dense(1024, activation = 'sigmoid')(x)

predictions = Dense(n_classes, activation='sigmoid')(x)
    
     model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss=keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True, alpha = 0.05, gamma = 12), optimizer=keras.optimizers.Adam(learning_rate=0.1))
   
    
Area under the curve:  0.3488372
Precision:  0.049804077
Recall:  0.762956
Precision/Recall avg:  0.24227878

Added level of 1024 layers. Nearly no change
__________________________________________________________________________________________
=======
    x = Dense(128, activation='relu')(inputs)
    x = Dense(256, activation='relu')(x)
    x = Dense(512, activation='relu')(x)
#     x = Dense(1024, activation='relu')(x)
#     x = Dense(2048, activation='relu')(x)


    # Output layer - use multilabel classification
    predictions = Dense(n_classes, activation='sigmoid')(x)
    
    #Tie model together
    model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(),keras.metrics.Recall()])
    


Mean area under curve:  90.96400380134583
Mean precision:  89.7228729724884
Mean recall:  13.511694520711899
____________________________________________________

cv = KFold(n_splits=10, shuffle=True, random_state=1023)

n_classes = 731

    # Input layer
    input_shape = (40,)
    inputs = keras.Input(input_shape)

    # Hidden layers
    x = Dense(128, activation='relu')(inputs)
    x = Dense(256, activation='relu')(x)
    x = Dense(512, activation='relu')(x)
    x = Dense(1024, activation='relu')(x)
#     x = Dense(2048, activation='relu')(x)


    # Output layer - use multilabel classification
    predictions = Dense(n_classes, activation='sigmoid')(x)
    
    #Tie model together
    model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(),keras.metrics.Recall()])
    


Mean area under curve:  91.08613550662994
Mean precision:  95.40671169757843
Mean recall:  13.478828221559525
____________________________________________________

cv = KFold(n_splits=10, shuffle=True, random_state=1023)

n_classes = 731

    # Input layer
    input_shape = (40,)
    inputs = keras.Input(input_shape)

    # Hidden layers
    x = Dense(128, activation='relu')(inputs)
#   x = Dense(256, activation='relu')(x)
#   x = Dense(512, activation='relu')(x)
#   x = Dense(1024, activation='relu')(x)
    x = Dense(2048, activation='relu')(x)


    # Output layer - use multilabel classification
    predictions = Dense(n_classes, activation='sigmoid')(x)
    
    #Tie model together
    model = keras.Model(inputs = inputs, outputs = predictions)
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = [keras.metrics.AUC(), keras.metrics.Precision(),keras.metrics.Recall()])
    


Mean area under curve:  91.09784305095673
Mean precision:  95.6020587682724
Mean recall:  13.812834173440933
____________________________________________________

>>>>>>> Stashed changes
